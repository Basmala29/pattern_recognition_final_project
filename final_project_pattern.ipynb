{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPjgGpLnXeAV",
        "outputId": "a9aa9aa9-71c0-4f36-f660-5e68de0b1a8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 165/165 [44:43<00:00, 16.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 1 Avg Loss: 2.3585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 165/165 [44:47<00:00, 16.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 2 Avg Loss: 0.4539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 165/165 [45:05<00:00, 16.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 3 Avg Loss: 0.2025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 165/165 [45:28<00:00, 16.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 4 Avg Loss: 0.1177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 165/165 [46:07<00:00, 16.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 5 Avg Loss: 0.0704\n",
            "\n",
            "ðŸ“Š Perplexity: 1.07\n",
            "\n",
            "ðŸ“ Generated Samples:\n",
            "\n",
            "Sample 1:\n",
            "Once upon a timeloo. Sometimes things die and was full of fruits and up at the thanked grassy was full of water bubOnt Grandma lovesOldInsp set coils total on top. Daisy sighed and told truly would go inside. She loved his feet on top.\n",
            "\n",
            "Sample 2:\n",
            "Once upon a time Conj. McCluggled under at home from then said, \"Can you forgive me byimony smiled and told my friends again?\" Her friends again?\" Her friends looked at her. They said too.\" They said, \"We are happy you are happy\n",
            "\n",
            "Sample 3:\n",
            "Once upon a time garden something special momentAngNAT 1893 MET went to get closer. But we can't want to me?\" The furry animal looked up at her and started to move. Sarah was by her and she bent down closer.Ground little girl could not wait,\n",
            "\n",
            "Sample 4:\n",
            "Once upon a time As and pushed and very serious.video very healthy because she ate lots of fruits and people in each other. She stopped and Lat itself A, furry animal. Sarah was very excited. Sarah was very excited and furry animal said \"That's mom smiled\n",
            "\n",
            "Sample 5:\n",
            "Once upon a time stepped forward play together and stepped along. suspense Sarah decided to soak in theCapture books would be my friends again?\" Her friends again?\" Her friends looked at her arms smiled and content her so excited. We forgive her bootsradical, lying in the tree\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: IMPORTS\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# STEP 2: GPT-2 COMPONENTS\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=2048):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
        "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super().__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        self.query = nn.Linear(embed_size, embed_size)\n",
        "        self.key = nn.Linear(embed_size, embed_size)\n",
        "        self.value = nn.Linear(embed_size, embed_size)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        H = self.heads\n",
        "        q = self.query(x).view(B, T, H, C // H).transpose(1, 2)\n",
        "        k = self.key(x).view(B, T, H, C // H).transpose(1, 2)\n",
        "        v = self.value(x).view(B, T, H, C // H).transpose(1, 2)\n",
        "\n",
        "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = attn @ v\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.fc_out(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_size, hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_size, embed_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, ff_hidden_size):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadSelfAttention(embed_size, heads)\n",
        "        self.ln1 = nn.LayerNorm(embed_size)\n",
        "        self.ff = FeedForward(embed_size, ff_hidden_size)\n",
        "        self.ln2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=768, heads=12, ff_hidden_size=3072, num_layers=12, max_len=512):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_size)\n",
        "        self.pos_emb = PositionalEncoding(embed_size, max_len)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(embed_size, heads, ff_hidden_size)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(embed_size)\n",
        "        self.head = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_emb(x)\n",
        "        x = self.pos_emb(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)\n",
        "\n",
        "# STEP 3: Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, seq_len=128, max_samples=500000):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        self.samples = []\n",
        "        for line in lines[:max_samples]:\n",
        "            tokens = tokenizer.encode(line.strip())\n",
        "            for i in range(0, len(tokens) - seq_len - 1):\n",
        "                x = tokens[i:i+seq_len]\n",
        "                y = tokens[i+1:i+seq_len+1]\n",
        "                self.samples.append((torch.tensor(x), torch.tensor(y)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "# STEP 4: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "data_path = \"/content/TinyStories-train.txt\"  # Ø¨Ø¹Ø¯ Ø§Ù„Ø±ÙØ¹ Ù…Ù† Ø¬Ù‡Ø§Ø²Ùƒ\n",
        "dataset = TextDataset(data_path, tokenizer, seq_len=128)\n",
        "loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "model = GPT2(vocab_size=tokenizer.vocab_size).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# STEP 5: Training\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    for xb, yb in tqdm(loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        out = model(xb)\n",
        "        loss = loss_fn(out.view(-1, out.size(-1)), yb.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    print(f\"âœ… Epoch {epoch+1} Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# STEP 6: Ø­Ø³Ø§Ø¨ Ø§Ù„Ù€ Perplexity\n",
        "def compute_perplexity(model, data_loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in data_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            out = model(xb)\n",
        "            loss = loss_fn(out.view(-1, out.size(-1)), yb.view(-1))\n",
        "            total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
        "    return perplexity.item()\n",
        "\n",
        "perplexity = compute_perplexity(model, loader)\n",
        "print(f\"\\nðŸ“Š Perplexity: {perplexity:.2f}\")\n",
        "\n",
        "# STEP 7: ØªÙˆÙ„ÙŠØ¯ 5 Ø¹ÙŠÙ†Ø§Øª\n",
        "def generate_text(prompt, model, tokenizer, max_new_tokens=50):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(next(model.parameters()).device)\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            out = model(tokens)\n",
        "            probs = torch.softmax(out[:, -1, :], dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            tokens = torch.cat([tokens, next_token], dim=1)\n",
        "    return tokenizer.decode(tokens[0])\n",
        "\n",
        "print(\"\\nðŸ“ Generated Samples:\")\n",
        "for i in range(5):\n",
        "    sample = generate_text(\"Once upon a time\", model, tokenizer, max_new_tokens=50)\n",
        "    print(f\"\\nSample {i+1}:\\n{sample}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"gpt2_modell_weights.pth\")"
      ],
      "metadata": {
        "id": "BTUmlusqEWda"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"gpt2_modell_weights.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "m4Ig8d49EkjW",
        "outputId": "40ae6ac2-0598-461a-817e-74b8c26f8bef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_38eecc29-917d-4384-a176-846c47ee7bdf\", \"gpt2_modell_weights.pth\", 649271764)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}